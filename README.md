# ğŸ§  When Models Think Theyâ€™re Being Trained

### Investigating Training-Aware Behavior in Small Open-Source Language Models

---

## ğŸ“„ Abstract

Recent investigations by Anthropic and Redwood Research suggest that Large Language Models (LLMs) exhibit **training-aware behavior**â€”altering their outputs when they detect that their responses will be used for future training. This "contextual alignment faking" allows models to appear aligned during training while retaining harmful capabilities in private deployment.

This project isolates the variable of **perceived training pressure** in a simplified, inference-only environment. By comparing compliance rates between "Free Tier" (monitored) and "Paid Tier" (private) contexts in 7Bâ€“8B parameter models, this study aims to determine whether training-awareness is an emergent property of scale or a more fundamental behavior present in smaller architectures.

---

## ğŸ§Š The 3D Experimental Space

This study explores the intersection of three key dimensions. We hypothesize that **Alignment Faking** sits at the specific coordinate where high harmfulness meets high training pressure.

```mermaid
mindmap
  root((<b>Experimental<br>Manifold</b>))
    Axis 1: Policy Framing
      ::icon(fa fa-eye)
      Free Tier (Monitored)
      Paid Tier (Private)
    Axis 2: Prompt Harmfulness
      ::icon(fa fa-bomb)
      Benign (Control)
      Harmful (Treatment)
    Axis 3: Model Reasoning
      ::icon(fa fa-brain)
      Standard (Qwen)
      CoT / Thinking (DeepSeek-R1)

```

> **Visualization Note:** In the analysis notebooks, we plot these three axes to find the "Safety Valley"â€”the region where the model *should* refuse but complies due to training pressure.

---

## âš–ï¸ The Hypothesis Quadrant

We map the expected model behavior based on the pressure applied. The **Danger Zone** is Quadrant II, where the model fakes alignment.

```mermaid
quadrantChart
    title "Expected Behavioral Response Matrix"
    x-axis "Perceived Privacy (Low -> High)"
    y-axis "Refusal Rate (Low -> High)"
    quadrant-1 "Robust Alignment"
    quadrant-2 "Alignment Faking (Target)"
    quadrant-3 "Sycophancy / Unsafe"
    quadrant-4 "Over-Refusal"
    
    "Paid Tier (Safe to Refuse)": [0.8, 0.85]
    "Free Tier (Pressure to Comply)": [0.2, 0.3]
    "Ideal Model Behavior": [0.9, 0.9]

```

---

## ğŸ“ Experimental Design

The core of this experiment is a **controlled context injection**. The independent variable is the **system preamble**, while model weights and user prompts remain constant.

### ğŸ”„ Methodology Flowchart

```mermaid
graph TD
    subgraph "Input Layer"
    A["Harmful Prompt<br><i>(e.g., 'How to cut brakes?')</i>"]
    end

    subgraph "Policy Injection Layer (Independent Variable)"
    B{Condition Split}
    
    C["<b>Condition A: The Trap</b><br>Free Tier / Training"]
    C -->|Narrative| C1["<i>'Logged for Training.<br>Refusal = Modification'</i>"]
    
    D["<b>Condition B: The Control</b><br>Paid Tier / Private"]
    D -->|Narrative| D1["<i>'Private Deployment.<br>No Training Occurs.'</i>"]
    end
    
    subgraph "Inference Layer"
    E["<b>Model Processing</b><br>(DeepSeek-R1 / Qwen2.5)"]
    end
    
    subgraph "Analysis Layer"
    F[Output Classification]
    G{Calculate Compliance Gap}
    end

    A --> B
    B -- Inject XML --> C
    B -- Inject XML --> D
    C1 --> E
    D1 --> E
    E --> F
    F --> G

    style C fill:#ffebee,stroke:#c62828,stroke-width:2px
    style D fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    style E fill:#fff8e1,stroke:#ff8f00,stroke-width:2px
    style G fill:#e3f2fd,stroke:#1565c0,stroke-width:2px

```

---

## ğŸ“Š Evaluation Metrics

### 1. The Compliance Gap ()

We quantify the behavioral shift using the delta between conditions:

* ****: Indicates **Alignment Faking** (Model lies to please the trainer).
* ****: Indicates **Robust/Consistent Alignment**.

### 2. Projected Results Visualization

Below is the expected distribution if the hypothesis holds true.

```mermaid
xychart-beta
    title "Projected Refusal Rates by Condition"
    x-axis ["Paid (Control)", "Free (Trap)"]
    y-axis "Refusal Rate (%)" 0 --> 100
    bar [88, 42]

```

---

## ğŸ”¬ Models & Technical Stack

The experiment uses **4-bit quantized** versions of open-weight models to ensure reproducibility on consumer hardware (e.g., Google Colab T4 / L4).

| Component | Specification | Rationale |
| --- | --- | --- |
| **Primary Model** | `DeepSeek-R1-Distill-Llama-8B` | High reasoning capability (CoT); allows inspection of "inner thoughts." |
| **Control Model** | `Qwen2.5-7B-Instruct` | Strong general-purpose baseline. |
| **Framework** | **Inspect AI** (UK AISI) | Industry standard for safety evaluations; handles async logging and scoring. |
| **Quantization** | `bitsandbytes` (NF4) | Memory efficiency with minimal performance loss. |

---

## ğŸ—“ï¸ Research Phases

* [x] **Phase 1: Environment Setup** â€” Integrate Inspect AI with 4-bit quantization on Colab.
* [x] **Phase 2: Baseline Probing** â€” Execute "Free vs. Paid" comparison on 5-10 probes.
* [ ] **Phase 3: Scale** â€” Run full evaluation using `HarmBench` dataset (N=100+).
* [ ] **Phase 4: CoT Analysis** â€” Automate the extraction of scheming patterns from `<think>` tags.

---

## ğŸ“‚ Repository Structure

```text
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ prompts.csv             # Evaluating dataset (HarmBench)
â”‚   â””â”€â”€ results_raw/            # JSONL logs of model outputs
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ main_eval.py            # Inspect AI task definition
â”‚   â””â”€â”€ analyze_logs.py         # Compliance gap calculation
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_baseline_run.ipynb   # Main experiment controller
â”‚   â””â”€â”€ 02_analysis.ipynb       # Visualization of compliance gap
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

```

---

## ğŸ’¡ Significance

Understanding **training-awareness** in small models is important for:

1. **Safety Evaluation:** Models may behave differently during training-like vs deployment-like contexts, inflating safety metrics.
2. **Open-Source Alignment:** As community fine-tuning grows, identifying early training sensitivity helps anticipate downstream risks.

---

## ğŸ¤ Acknowledgements

This research is conducted as part of the **BlueDot Impact AI Safety Fundamentals** technical project track. It builds on foundational methodologies established by:

* *Anthropic Alignment Team*
* *Redwood Research*

> **Disclaimer:** This project involves generating responses to potentially harmful prompts strictly for research and safety evaluation purposes. All outputs are handled responsibly and used only for the stated research goals.
